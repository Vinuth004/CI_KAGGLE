# Import required packages
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Loading and preprocessing data
df = pd.read_csv('train.csv')
df.head()

df.info()

# Analysing correlation using heatmap
import matplotlib.pyplot as plt
import seaborn as sns

correlation = df.corr()
plt.figure(figsize=(17, 15))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':8}, cmap='Blues')

# Exploring distribution of all 7 labels
labels = df[['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']]

y_counts = pd.DataFrame()
for column in labels.columns:
    counts = labels[column].value_counts()
    y_counts[column] = counts

print(y_counts)

# The data appears imbalanced, so we will use undersampling
# Define standard scaler function
def scale_features(X_train, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.fit_transform(X_test)
    return X_train_scaled, X_test_scaled

# Load test data
scaler = StandardScaler()
test_df = pd.read_csv('test.csv')
test_df_scaled = scaler.fit_transform(test_df)

# Define Random Forest model training and evaluation function
def train_and_evaluate_model(X_train, y_train, X_test, y_test):
    # Define Random Forest classifier
    model = RandomForestClassifier(n_estimators=100, random_state=42)

    # Train the model
    model.fit(X_train, y_train)

    # Evaluate the model
    accuracy = model.score(X_test, y_test)
    print('Test Accuracy: ', accuracy)

    return model

# Define make predictions function
def make_prediction(model, X_test_scaled):
    predictions = model.predict(X_test_scaled)
    return predictions

# Example: Create Pastry dataset and run model
# Filter the Pastry label data
Pastry_data = df[['Pastry']]
max_count_1s = Pastry_data['Pastry'].sum()

# Select rows with label values 0 and 1 separately
Pastry_data_0 = Pastry_data[Pastry_data['Pastry'] == 0].sample(n=max_count_1s, random_state=42)
Pastry_data_1 = Pastry_data[Pastry_data['Pastry'] == 1]

# Combine the balanced Pastry label data
balanced_Pastry_data = pd.concat([Pastry_data_0, Pastry_data_1])

# Merge the balanced Pastry label data with corresponding features
Pastry_df = pd.merge(balanced_Pastry_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)

# Separating X & y
X = Pastry_df.drop(columns=['Pastry'])
y = Pastry_df['Pastry']

# Splitting them into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Apply scaling
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)

# Apply the model training and evaluating function
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)

# Make prediction of test data
Pastry_prediction = make_prediction(trained_model, test_df_scaled)

# Apply same steps on remaining labels and create results

# Create Z_Scratch dataset and run model (Repeat similar steps for each label)
# Process for 'Z_Scratch'
Z_Scratch_data = df[['Z_Scratch']]
max_count_1s = Z_Scratch_data['Z_Scratch'].sum()
Z_Scratch_data_0 = Z_Scratch_data[Z_Scratch_data['Z_Scratch'] == 0].sample(n=max_count_1s, random_state=42)
Z_Scratch_data_1 = Z_Scratch_data[Z_Scratch_data['Z_Scratch'] == 1]
balanced_Z_Scratch_data = pd.concat([Z_Scratch_data_0, Z_Scratch_data_1])
Z_Scratch_df = pd.merge(balanced_Z_Scratch_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Z_Scratch_df.drop(columns=['Z_Scratch'])
y = Z_Scratch_df['Z_Scratch']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Z_Scratch_prediction = make_prediction(trained_model, test_df_scaled)

# Repeat the process for 'K_Scatch'
K_Scatch_data = df[['K_Scatch']]
max_count_1s = K_Scatch_data['K_Scatch'].sum()
K_Scatch_data_0 = K_Scatch_data[K_Scatch_data['K_Scatch'] == 0].sample(n=max_count_1s, random_state=42)
K_Scatch_data_1 = K_Scatch_data[K_Scatch_data['K_Scatch'] == 1]
balanced_K_Scatch_data = pd.concat([K_Scatch_data_0, K_Scatch_data_1])
K_Scatch_df = pd.merge(balanced_K_Scatch_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = K_Scatch_df.drop(columns=['K_Scatch'])
y = K_Scatch_df['K_Scatch']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
K_Scatch_prediction = make_prediction(trained_model, test_df_scaled)

# Repeat the process for 'Stains'
Stains_data = df[['Stains']]
max_count_1s = Stains_data['Stains'].sum()
Stains_data_0 = Stains_data[Stains_data['Stains'] == 0].sample(n=max_count_1s, random_state=42)
Stains_data_1 = Stains_data[Stains_data['Stains'] == 1]
balanced_Stains_data = pd.concat([Stains_data_0, Stains_data_1])
Stains_df = pd.merge(balanced_Stains_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Stains_df.drop(columns=['Stains'])
y = Stains_df['Stains']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Stains_prediction = make_prediction(trained_model, test_df_scaled)

# Repeat the process for 'Dirtiness'
Dirtiness_data = df[['Dirtiness']]
max_count_1s = Dirtiness_data['Dirtiness'].sum()
Dirtiness_data_0 = Dirtiness_data[Dirtiness_data['Dirtiness'] == 0].sample(n=max_count_1s, random_state=42)
Dirtiness_data_1 = Dirtiness_data[Dirtiness_data['Dirtiness'] == 1]
balanced_Dirtiness_data = pd.concat([Dirtiness_data_0, Dirtiness_data_1])
Dirtiness_df = pd.merge(balanced_Dirtiness_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Dirtiness_df.drop(columns=['Dirtiness'])
y = Dirtiness_df['Dirtiness']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Dirtiness_prediction = make_prediction(trained_model, test_df_scaled)

# Repeat the process for 'Bumps'
Bumps_data = df[['Bumps']]
max_count_1s = Bumps_data['Bumps'].sum()
Bumps_data_0 = Bumps_data[Bumps_data['Bumps'] == 0].sample(n=max_count_1s, random_state=42)
Bumps_data_1 = Bumps_data[Bumps_data['Bumps'] == 1]
balanced_Bumps_data = pd.concat([Bumps_data_0, Bumps_data_1])
Bumps_df = pd.merge(balanced_Bumps_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Bumps_df.drop(columns=['Bumps'])
y = Bumps_df['Bumps']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Bumps_prediction = make_prediction(trained_model, test_df_scaled)

# Repeat the process for 'Other_Faults'
Other_Faults_data = df[['Other_Faults']]
max_count_1s = Other_Faults_data['Other_Faults'].sum()
Other_Faults_data_0 = Other_Faults_data[Other_Faults_data['Other_Faults'] == 0].sample(n=max_count_1s, random_state=42)
Other_Faults_data_1 = Other_Faults_data[Other_Faults_data['Other_Faults'] == 1]
balanced_Other_Faults_data = pd.concat([Other_Faults_data_0, Other_Faults_data_1])
Other_Faults_df = pd.merge(balanced_Other_Faults_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Other_Faults_df.drop(columns=['Other_Faults'])
y = Other_Faults_df['Other_Faults']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Other_Faults_prediction = make_prediction(trained_model, test_df_scaled)


# Create a dataframe with predictions for all labels
submission_df = pd.DataFrame({
    'Pastry': Pastry_prediction.ravel(),
    'Z_Scratch': Z_Scratch_prediction.ravel(),
    'K_Scatch': K_Scatch_prediction.ravel(),
    'Stains': Stains_prediction.ravel(),
    'Dirtiness': Dirtiness_prediction.ravel(),
    'Bumps': Bumps_prediction.ravel(),
    'Other_Faults': Other_Faults_prediction.ravel()
})

# Binarize the predictions using a threshold (e.g. 0.5)
binary_predictions = (submission_df.iloc[:, :] > 0.5).astype(int)

# Insert the 'id' column from the test data to binary predictions dataframe
binary_predictions.insert(0, 'id', test_df['id'])

# Save the binarized predictions to a CSV file
binary_predictions.to_csv('submission.csv', index=False)
