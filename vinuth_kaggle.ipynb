# Import required packages
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA


# Loading and preprocessing data
df = pd.read_csv('train.csv')
df.head()

df.info()

# Analyzing correlation using heatmap
correlation = df.corr()
plt.figure(figsize=(17, 15))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':8}, cmap='Blues')

# Exploring distribution of all 7 labels
labels = df[['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']]

y_counts = pd.DataFrame()
for column in labels.columns:
    counts = labels[column].value_counts()
    y_counts[column] = counts

print(y_counts)

#Pairplot
selected_features = ['X_Minimum', 'Y_Minimum', 'Steel_Plate_Thickness', 'Sum_of_Luminosity']
sns.pairplot(df[selected_features + ['Pastry']], hue='Pastry', diag_kind='kde')
plt.suptitle('Pairplot of Selected Features', y=1.02)
plt.show()

#Boxplot
plt.figure(figsize=(14, 8))
sns.boxplot(data=df[selected_features], orient='h', palette='Set3')
plt.title('Boxplot for Selected Features')
plt.show()

#PCA
pca = PCA(n_components=2)
components = pca.fit_transform(df.drop(columns=['Pastry']))
plt.figure(figsize=(10, 6))
plt.scatter(components[:, 0], components[:, 1], c=df['Pastry'], cmap='viridis', edgecolor='k', s=40)
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('PCA of Features')
plt.colorbar()
plt.show()

# Sample DataFrame setup for barplot with hue
for label in labels:
    plt.figure(figsize=(8, 6))
    # Create a DataFrame that includes a 'hue' column for proper color coding
    counts = df[label].value_counts().reset_index()
    counts.columns = [label, 'Count']
    counts['Category'] = counts[label]  # Create a 'Category' column for hue

    sns.barplot(x=label, y='Count', data=counts, hue='Category', palette='viridis')
    plt.xlabel(label)
    plt.ylabel('Count')
    plt.title(f'Count of {label}')
    plt.show()


# Pie Charts for each label
labels = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']
for label in labels:
    plt.figure(figsize=(8, 8))
    counts = df[label].value_counts()
    plt.pie(counts, labels=counts.index, autopct='%1.1f%%', colors=['#ff9999','#66b3ff'], startangle=140)
    plt.title(f'Distribution of {label}')
    plt.show()

# The data appears imbalanced, so we will use undersampling
# Define standard scaler function
def scale_features(X_train, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled

# Load test data
scaler = StandardScaler()
test_df = pd.read_csv('test.csv')
test_df_scaled = scaler.fit_transform(test_df)

# Define Random Forest model training and evaluation function with hyperparameter tuning
def train_and_evaluate_model(X_train, y_train, X_test, y_test):
    # Define the parameter grid for Random Forest
    param_grid = {
        'n_estimators': [20],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10],
        'max_features': ['sqrt', 'log2', None]
    }

    # Use GridSearchCV to find the best parameters
    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)

    # Train the model with the best parameters
    best_model = grid_search.best_estimator_
    best_model.fit(X_train, y_train)

    # Evaluate the model
    accuracy = best_model.score(X_test, y_test)
    print('Test Accuracy with optimized hyperparameters: ', accuracy)

    return best_model

# Define make predictions function
def make_prediction(model, X_test_scaled):
    predictions = model.predict(X_test_scaled)
    return predictions

# Process for 'Pastry'
Pastry_data = df[['Pastry']]
max_count_1s = Pastry_data['Pastry'].sum()
Pastry_data_0 = Pastry_data[Pastry_data['Pastry'] == 0].sample(n=max_count_1s, random_state=42)
Pastry_data_1 = Pastry_data[Pastry_data['Pastry'] == 1]
balanced_Pastry_data = pd.concat([Pastry_data_0, Pastry_data_1])
Pastry_df = pd.merge(balanced_Pastry_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Pastry_df.drop(columns=['Pastry'])
y = Pastry_df['Pastry']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Pastry_prediction = make_prediction(trained_model, test_df_scaled)

# Process for 'Z_Scratch'
Z_Scratch_data = df[['Z_Scratch']]
max_count_1s = Z_Scratch_data['Z_Scratch'].sum()
Z_Scratch_data_0 = Z_Scratch_data[Z_Scratch_data['Z_Scratch'] == 0].sample(n=max_count_1s, random_state=42)
Z_Scratch_data_1 = Z_Scratch_data[Z_Scratch_data['Z_Scratch'] == 1]
balanced_Z_Scratch_data = pd.concat([Z_Scratch_data_0, Z_Scratch_data_1])
Z_Scratch_df = pd.merge(balanced_Z_Scratch_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Z_Scratch_df.drop(columns=['Z_Scratch'])
y = Z_Scratch_df['Z_Scratch']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Z_Scratch_prediction = make_prediction(trained_model, test_df_scaled)

# Process for 'K_Scatch'
K_Scatch_data = df[['K_Scatch']]
max_count_1s = K_Scatch_data['K_Scatch'].sum()
K_Scatch_data_0 = K_Scatch_data[K_Scatch_data['K_Scatch'] == 0].sample(n=max_count_1s, random_state=42)
K_Scatch_data_1 = K_Scatch_data[K_Scatch_data['K_Scatch'] == 1]
balanced_K_Scatch_data = pd.concat([K_Scatch_data_0, K_Scatch_data_1])
K_Scatch_df = pd.merge(balanced_K_Scatch_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = K_Scatch_df.drop(columns=['K_Scatch'])
y = K_Scatch_df['K_Scatch']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
K_Scatch_prediction = make_prediction(trained_model, test_df_scaled)

# Process for 'Stains'
Stains_data = df[['Stains']]
max_count_1s = Stains_data['Stains'].sum()
Stains_data_0 = Stains_data[Stains_data['Stains'] == 0].sample(n=max_count_1s, random_state=42)
Stains_data_1 = Stains_data[Stains_data['Stains'] == 1]
balanced_Stains_data = pd.concat([Stains_data_0, Stains_data_1])
Stains_df = pd.merge(balanced_Stains_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Stains_df.drop(columns=['Stains'])
y = Stains_df['Stains']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Stains_prediction = make_prediction(trained_model, test_df_scaled)

# Process for 'Dirtiness'
Dirtiness_data = df[['Dirtiness']]
max_count_1s = Dirtiness_data['Dirtiness'].sum()
Dirtiness_data_0 = Dirtiness_data[Dirtiness_data['Dirtiness'] == 0].sample(n=max_count_1s, random_state=42)
Dirtiness_data_1 = Dirtiness_data[Dirtiness_data['Dirtiness'] == 1]
balanced_Dirtiness_data = pd.concat([Dirtiness_data_0, Dirtiness_data_1])
Dirtiness_df = pd.merge(balanced_Dirtiness_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Dirtiness_df.drop(columns=['Dirtiness'])
y = Dirtiness_df['Dirtiness']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Dirtiness_prediction = make_prediction(trained_model, test_df_scaled)

# Process for 'Bumps'
Bumps_data = df[['Bumps']]
max_count_1s = Bumps_data['Bumps'].sum()
Bumps_data_0 = Bumps_data[Bumps_data['Bumps'] == 0].sample(n=max_count_1s, random_state=42)
Bumps_data_1 = Bumps_data[Bumps_data['Bumps'] == 1]
balanced_Bumps_data = pd.concat([Bumps_data_0, Bumps_data_1])
Bumps_df = pd.merge(balanced_Bumps_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Bumps_df.drop(columns=['Bumps'])
y = Bumps_df['Bumps']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Bumps_prediction = make_prediction(trained_model, test_df_scaled)

# Process for 'Other_Faults'
Other_Faults_data = df[['Other_Faults']]
max_count_1s = Other_Faults_data['Other_Faults'].sum()
Other_Faults_data_0 = Other_Faults_data[Other_Faults_data['Other_Faults'] == 0].sample(n=max_count_1s, random_state=42)
Other_Faults_data_1 = Other_Faults_data[Other_Faults_data['Other_Faults'] == 1]
balanced_Other_Faults_data = pd.concat([Other_Faults_data_0, Other_Faults_data_1])
Other_Faults_df = pd.merge(balanced_Other_Faults_data, df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']), left_index=True, right_index=True)
X = Other_Faults_df.drop(columns=['Other_Faults'])
y = Other_Faults_df['Other_Faults']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
Other_Faults_prediction = make_prediction(trained_model, test_df_scaled)

final_predictions = pd.DataFrame({
    'Pastry': Pastry_prediction,
    'Z_Scratch': Z_Scratch_prediction,
    'K_Scatch': K_Scatch_prediction,
    'Stains': Stains_prediction,
    'Dirtiness': Dirtiness_prediction,
    'Bumps': Bumps_prediction,
    'Other_Faults': Other_Faults_prediction
})

final_predictions.insert(0, 'id', test_df['id'])

# Save to CSV file
final_predictions.to_csv('submission.csv', index=False)

# Reorder columns if necessary (if 'Id' needs to be the first column)
final_predictions = final_predictions[['id', 'Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']]

# Save to CSV file
final_predictions.to_csv('submission.csv', index=False)
