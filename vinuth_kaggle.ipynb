# Install required packages
!pip install streamlit scikit-learn pandas numpy matplotlib seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
import pickle
import os

# Load dataset
df = pd.read_csv('train.csv')

# Ensure 'id' or other non-feature columns are not included
if 'id' in df.columns:
    df = df.drop(columns=['id'])

# Define features and labels
labels = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']
features = df.drop(columns=labels)

# EDA Part
print(df.head())
print(df.info())

# Analyzing correlation using heatmap
correlation = df.corr()
plt.figure(figsize=(17, 15))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':8}, cmap='Blues')
plt.show()

# Exploring distribution of all 7 labels
y_counts = pd.DataFrame()
for column in labels:
    counts = df[column].value_counts()
    y_counts[column] = counts
print(y_counts)

# Pairplot
selected_features = ['X_Minimum', 'Y_Minimum', 'Steel_Plate_Thickness', 'Sum_of_Luminosity']
sns.pairplot(df[selected_features + ['Pastry']], hue='Pastry', diag_kind='kde')
plt.suptitle('Pairplot of Selected Features', y=1.02)
plt.show()

# Boxplot
plt.figure(figsize=(14, 8))
sns.boxplot(data=df[selected_features], orient='h', palette='Set3')
plt.title('Boxplot for Selected Features')
plt.show()

# PCA
pca = PCA(n_components=2)
components = pca.fit_transform(df.drop(columns=['Pastry']))
plt.figure(figsize=(10, 6))
plt.scatter(components[:, 0], components[:, 1], c=df['Pastry'], cmap='viridis', edgecolor='k', s=40)
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('PCA of Features')
plt.colorbar()
plt.show()

# Pie Charts for each label
labels = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']
for label in labels:
    plt.figure(figsize=(8, 8))
    counts = df[label].value_counts()
    plt.pie(counts, labels=counts.index, autopct='%1.1f%%', colors=['#ff9999','#66b3ff'], startangle=140)
    plt.title(f'Distribution of {label}')
    plt.show()

# Data appears imbalanced, so will use undersampling
def scale_features(X_train, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled

# Model training with hyperparameter tuning
def train_and_evaluate_model(X_train, y_train, X_test, y_test, model_name):
    param_grid = {
        'n_estimators': [20],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10],
        'max_features': ['sqrt', 'log2', None]
    }

    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    best_model.fit(X_train, y_train)

    accuracy = best_model.score(X_test, y_test)
    print(f'Test Accuracy with optimized hyperparameters for {model_name}: ', accuracy)

    # Save the best model using pickle
    if not os.path.exists('models'):
        os.makedirs('models')
    with open(f'models/{model_name}_model.pkl', 'wb') as file:
        pickle.dump(best_model, file)

    return best_model

# Function to load the model
def load_model(model_name):
    with open(f'models/{model_name}_model.pkl', 'rb') as file:
        model = pickle.load(file)
    return model

# Function to process each label
def process_label(label, df):
    data = df[[label]]
    max_count_1s = data[label].sum()
    data_0 = data[data[label] == 0].sample(n=max_count_1s, random_state=42)
    data_1 = data[data[label] == 1]
    balanced_data = pd.concat([data_0, data_1])
    
    df_subset = pd.merge(balanced_data, features, left_index=True, right_index=True)
    X = df_subset.drop(columns=[label])
    y = df_subset[label]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
    
    # Check if the model already exists
    if os.path.exists(f'models/{label}_model.pkl'):
        trained_model = load_model(label)
        print(f'Loaded saved model for {label}')
    else:
        trained_model = train_and_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test, label)
        print(f'Saved trained model for {label}')
    
    return trained_model

# Train and save models for each label
models = {}
for label in labels:
    models[label] = process_label(label, df)

# Install Streamlit for creating the web app
!pip install streamlit

import streamlit as st
import pickle

# Load the trained models
models = {}
for label in labels:
    with open(f'models/{label}_model.pkl', 'rb') as file:
        models[label] = pickle.load(file)

# Load test data
test_df = pd.read_csv('test.csv')

# Remove 'id' column before scaling and prediction
test_df_features = test_df.drop(columns=['id'])

# Scale the test data
scaler = StandardScaler()
test_df_scaled = scaler.fit_transform(test_df_features)

# Define make predictions function
def make_prediction(model, X_test_scaled):
    predictions = model.predict(X_test_scaled)
    return predictions

# Make predictions for all labels
final_predictions = pd.DataFrame()
final_predictions['id'] = test_df['id']

for label in labels:
    predictions = make_prediction(models[label], test_df_scaled)
    final_predictions[label] = predictions

# Save the final predictions to a CSV file
final_predictions.to_csv('submission.csv', index=False)

# Define the Streamlit app
st.title("Steel Plate Defect Prediction")

st.header("Input Features")

if st.button('Predict'):
    input_data = np.array([])  # Update this with actual input data collection from Streamlit input elements
    predictions = {}
    
    for label, model in models.items():
        prediction = model.predict([input_data])
        predictions[label] = 'Defective' if prediction[0] == 1 else 'Not Defective'
    
    st.subheader("Prediction Results:")
    for label, result in predictions.items():
        st.write(f"{label}: {result}")

!pip install streamlit ngrok scikit-learn pandas numpy matplotlib seaborn pyngrok


# Write the Streamlit app code to app.py
with open("app.py", "w") as file:
    file.write("""
import streamlit as st
import pickle

# Load the trained models
models = {}
labels = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']

for label in labels:
    with open(f'models/{label}_model.pkl', 'rb') as file:
        models[label] = pickle.load(file)

# Define the Streamlit app
st.title("Steel Plate Defect Prediction")

st.header("Input Features")
X_Minimum = st.number_input("X_Minimum")
X_Maximum = st.number_input("X_Maximum")
Y_Minimum = st.number_input("Y_Minimum")
Y_Maximum = st.number_input("Y_Maximum")
Pixels_Areas = st.number_input("Pixels_Areas")
X_Perimeter = st.number_input("X_Perimeter")
Y_Perimeter = st.number_input("Y_Perimeter")
Sum_of_Luminosity = st.number_input("Sum_of_Luminosity")
Minimum_of_Luminosity = st.number_input("Minimum_of_Luminosity")
Maximum_of_Luminosity = st.number_input("Maximum_of_Luminosity")
Length_of_Conveyer = st.number_input("Length_of_Conveyer")
TypeOfSteel_A300 = st.number_input("TypeOfSteel_A300")
TypeOfSteel_A400 = st.number_input("TypeOfSteel_A400")
Steel_Plate_Thickness = st.number_input("Steel_Plate_Thickness")
Edges_Index = st.number_input("Edges_Index")
Empty_Index = st.number_input("Empty_Index")
Square_Index = st.number_input("Square_Index")
Outside_X_Index = st.number_input("Outside_X_Index")
Edges_X_Index = st.number_input("Edges_X_Index")
Edges_Y_Index = st.number_input("Edges_Y_Index")
Outside_Global_Index = st.number_input("Outside_Global_Index")
LogOfAreas = st.number_input("LogOfAreas")
Log_X_Index = st.number_input("Log_X_Index")
Log_Y_Index = st.number_input("Log_Y_Index")
Orientation_Index = st.number_input("Orientation_Index")
Luminosity_Index = st.number_input("Luminosity_Index")
SigmoidOfAreas = st.number_input("SigmoidOfAreas")

# Store all inputs into a list
input_data = [
    X_Minimum, X_Maximum, Y_Minimum, Y_Maximum, Pixels_Areas, X_Perimeter,
    Y_Perimeter, Sum_of_Luminosity, Minimum_of_Luminosity, Maximum_of_Luminosity,
    Length_of_Conveyer, TypeOfSteel_A300, TypeOfSteel_A400, Steel_Plate_Thickness,
    Edges_Index, Empty_Index, Square_Index, Outside_X_Index, Edges_X_Index,
    Edges_Y_Index, Outside_Global_Index, LogOfAreas, Log_X_Index, Log_Y_Index,
    Orientation_Index, Luminosity_Index, SigmoidOfAreas
]

if st.button('Predict'):
    predictions = {}
    
    for label, model in models.items():
        prediction = model.predict([input_data])
        predictions[label] = 'Defective' if prediction[0] == 1 else 'Not Defective'
    
    st.subheader("Prediction Results:")
    for label, result in predictions.items():
        st.write(f"{label}: {result}")
    """)

print("app.py file created.")

from pyngrok import ngrok

# Terminate any open tunnels if running multiple times
ngrok.kill()

# Set the authtoken for ngrok
ngrok.set_auth_token("2lDSgmvZsTf7BR6rs9FMJ4L6ZHC_5EWwvCNXumnmLpB5E6izX")

# Correct way to set up the ngrok tunnel with explicit HTTP protocol and port number
public_url = ngrok.connect(8501, "http")  # Explicitly specify the port number and protocol
print(f'Public URL: {public_url}')

# Run the Streamlit app
!streamlit run app.py &
